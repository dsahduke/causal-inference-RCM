{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Correlation is not equal to causation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have seen this phrase before. It usually refers to the fact that just because we observe trends in data does not necessarily mean that one variable in the data *causes* changes in the other. There are many humorous examples of this, and the following provides many such examples:\n",
    "\n",
    "https://www.tylervigen.com/spurious-correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are often interested in answering causal questions, especially when it comes to health. Does smoking cause cancer? Does exercise decrease the risk of heart disease? Questions such as these all require causal analysis to answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal inference is the branch of statistics which deals with these questions. The gold standard tool of causal inference is *randomization*, which is typically employed in randomized control trials (RCTs). In RCTs, subjects are randomized into 2 groups, one which receives an intervention and one which does not. Then, whatever effect is observed can be attributed to the intervention. \n",
    "\n",
    "However, what happens if we do not have a randomized control trial? How can we still make inferences about causal effects? We will look at this question in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have built a model which identifies patients who are likely to benefit from palliative care. Whenever such a patient is identified in the hospital, the doctor is notified by email that their patient may benefit. We want to know whether this notification has a causal effect on certain outcomes, such as the percentage of patients who recieve palliative care services.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this seems like a location where a randomized control trial would be perfect, randomized control trials are \n",
    "\n",
    "    1. Expensive\n",
    "    2. Time-consuming\n",
    "    3. Ethically complicated\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can somehow mimic the results of a randomized control trial, while using observational data, that would be a good start towards showing that this intervention had a real effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubin Causal Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Rubin, in his seminal papers in '74 and '78, defined a new framework for doing causal inference. There is also work done by Judea Pearl, which is another popular framework, but which we will not discuss in this class. In his model, Rubin relies on a concept known as the *Potential Outcome*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **potential outcome** is the value of a unit's measurement of interest after treatment or non-treatment. However, the fundamental problem in causal inference is that we can only observe one of these measurements! Either we get the value after treatment or if a unit does not receive the treatment. So essentially this boils down to a missing data problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of RCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can just go ahead and compute the causal effect of some intervention, we have to make sure that we meet certain assumptions. These are very important in the RCM since if any of these are violated, the entire analysis could be invalid. The most important assumption is the SUTVA assumption, which stands for Stable Unit Treatment Value Assumption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUTVA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumption states that the potential outcomes for any unit do not vary with the treatments assigned to any other units AND that there are no different versions of the treatment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some studies, it is very easy to violate these assumptions. For example, if you are studying the effect of some contagious disease, it is clear that the the potential outcome for one unit could vary with another since it may make them more/less likely to spread the disease. There is still ongoing research to determine whether other assumptions are required if SUTVA is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, if we are working with observational data, we need a few more assumptions. In particular, we do not control or know the assignment mechanism. That is, we have no idea why certain units received the treatment and others did not. In order to account for this, we need the following properties to hold:\n",
    "\n",
    "**Unconfoundedness**\n",
    "        - The probability of receiving the treatment, after knowing the values of covariates, is independent of the potential outcomes\n",
    "\n",
    "$$ Pr(\\textbf{W}|\\textbf{Y}(0), \\textbf{Y}(1), \\textbf{X}) = Pr(\\textbf{W}|\\textbf{X})$$\n",
    "\n",
    "Where $\\textbf{W}$ is the treatment vector, $\\textbf{Y}(0), \\textbf{Y}(1)$ are potential outcomes, and $\\textbf{X}$ are the covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that whether you are in the treatment group can be determined by your covariates. If this were not the case, and it depended on the potential outcome, then we have no way of splitting up the two groups since we only observe at most 1 potential outcome! Therefore, there is missing information that is affecting our treatment assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probabilistic Overlap** The probability of receiving the treatment is between 0 and 1 for every unit. \n",
    "\n",
    "$$0 < Pr(W_i = 1|X_i) < 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, these two assumptions make up the **strong ignorability** assumption "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Randomization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is randomization an important tool in causal inference? Formally, randomization, on average, *balances covariates*. This means that the distribution of covariates in the treatment group and the intervention group are similar. When we randomize, we are essentially drawing from the total population in both the treatment and intervention groups, so it makes sense that these would be balanced. If we could, we would randomize every experiment and not have to deal with many of the assumptions stated above. However, our goal with the following methods is to *mimic* randomization. The key here is that we need balanced covariates in both groups, since this is the primary aim of randomization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumptions above allow us to use methods that mimic randomization and create a *psuedo-population* which has balanced covariates.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing Covariates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have strong ignorability, we can do valid causal inference if we compare the difference in Y between the treatment group and control group provided that the covariates are balanced. So how exactly do we balance these covariates? When we have a small number of covariates, we can take a matching or stratification approach.\n",
    "\n",
    "In an extreme case (where this is possible), we can use *exact matching*. That is, for every unit in the treatment group, find a control group member with the exact same covariates. If we do this, then we force the covariates to be balanced (actually identical). Obviously this is not possible in most cases and represents an ideal case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our example of palliative care notifications. We're going to look at an abbreviated form of the data to see if we can see whether the covariates are balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notification_df = pd.read_csv(\"./assets/causal_inf_extract.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notification_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 2 groups: those that did not receive the notification (False) and those that did (True). We also have a few covariates (though there were many others used in this analysis). We will look at the distributions of these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = notification_df, x='age', hue='notification', stat='density', bins=50, common_norm=False)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.title(\"Distribution of Age across groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(notification_df['notification'], notification_df['admission_type_label_Urgent Admission'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5/340)\n",
    "print(4127/(4127+34463))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(notification_df['notification'], notification_df['admission_type_label_Routine Elective Admission'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(notification_df['notification'], notification_df['admission_type_label_Emergency Admission'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest-Neighbor Matching "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to find the effect of the notifications on the `acp_note_filed` column. If we observe the empirical difference without balancing covariates, we see the following relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(notification_df['notification'], notification_df['acp_note_filed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clearly, this is a pretty large distance. However, as we have seen, it isn't enough to simply do this type of analysis, since we are not controlling for the covariates. We will continue with a nearest neighbor matching algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define a distance metric so that we can compute the distance between data points. We will use the Mahalanobic distance in order to do this:\n",
    "\n",
    "#### Mahalanobis Distance (1936)\n",
    "$$d(\\textbf{x}, \\textbf{y}) = \\sqrt{(\\textbf{x} - \\textbf{y})^TS^{-1}(\\textbf{x} - \\textbf{y})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\textbf{x}$ and $\\textbf{y}$ are two vectors from the same distribution and S is the covariance matrix. This allows for a generalized euclidean distance in high dimensions. Let's write a function to calculate this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S = np.cov(notification_df.loc[:, [\n",
    "                                    'age', \n",
    "                                    'admission_type_label_Urgent Admission',\n",
    "                                    'admission_type_label_Routine Elective Admission',\n",
    "                                    'admission_type_label_Emergency Admission'\n",
    "                                    ]].values.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_distance(x, y, S):\n",
    "    \"\"\"Computes the mahalanobis distance between 2 vectors\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        a vector to be compared to y\n",
    "    y : np.ndarray\n",
    "        a vector from the same distribution as x\n",
    "    S : np.ndarray\n",
    "        The covariance matrix for the data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d : np.float32\n",
    "        The distance between x and y\n",
    "    \"\"\"\n",
    "    left_term = (x - y).T\n",
    "    inv_cov = np.linalg.inv(S)\n",
    "    right_term = (x - y)\n",
    "    left_term = np.dot(left_term, inv_cov)\n",
    "    d = np.sqrt(np.dot(left_term, right_term))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find close matches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each treatment unit, we want to find the *k* closest neighbors in the negative_df and take their average status for `acp_note_filed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = notification_df.loc[notification_df['notification'] == True, [\n",
    "                                    'age', \n",
    "                                    'admission_type_label_Urgent Admission',\n",
    "                                    'admission_type_label_Routine Elective Admission',\n",
    "                                    'admission_type_label_Emergency Admission']]\n",
    "\n",
    "negative_df = notification_df.loc[notification_df['notification'] == False, [\n",
    "                                    'age', \n",
    "                                    'admission_type_label_Urgent Admission',\n",
    "                                    'admission_type_label_Routine Elective Admission',\n",
    "                                    'admission_type_label_Emergency Admission']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "positive_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dict = {}\n",
    "for ind in positive_df.index:\n",
    "    temp_index = negative_df.apply(mahalanobis, \n",
    "                      v = positive_df.loc[ind, :].values, \n",
    "                      VI=np.linalg.inv(S), \n",
    "                      axis=1).sort_values().index[0:3]\n",
    "    match_dict[ind] = list(temp_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_outcome(notification_df, indices):\n",
    "    \"\"\"Get average outcome given a set of indices\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.mean(notification_df.loc[indices, 'acp_note_filed'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a new dictionary where the keys are the index of positive df\n",
    "### and the values are returned by get_average_outcome\n",
    "potential_outcome_dict = {k: get_average_outcome(notification_df, v) for (k,v) in match_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_outcome_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "po_df = pd.DataFrame(pd.Series(potential_outcome_dict))\n",
    "po_df.columns = ['potential_outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "po_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "notification_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notification_df = pd.merge(notification_df, po_df, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the potential outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notification_df.loc[notification_df['notification'] == True, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_treatment_effect = (notification_df.loc[notification_df['notification'] == True, 'acp_note_filed'] \n",
    "                            - notification_df.loc[notification_df['notification'] == True, 'potential_outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(average_treatment_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The potential outcome is now the average of the nearest neighbors. We can then assess the causal effect as the average difference between units' potential outcomes under treatment versus controlled!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
